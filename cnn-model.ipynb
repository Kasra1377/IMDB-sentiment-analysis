{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn-model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1uKEm-V4fM7QaCkub2H0eJZEGSsVWGBGn","authorship_tag":"ABX9TyOqRJApnjytnc5JJVsqwQ1u"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLFblA2LSvDB","executionInfo":{"status":"ok","timestamp":1634748210227,"user_tz":-210,"elapsed":7590,"user":{"displayName":"Kasra Sadeghianpoor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-M-IQfCdV1sDtftP0aqLtsS4eCJ8Pzr5Ka4Ee=s64","userId":"12889102283785354184"}},"outputId":"dfeed6ee-5398-4365-8d30-2a0b330c066f"},"source":["!pip install shap"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shap\n","  Downloading shap-0.39.0.tar.gz (356 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30 kB 35.3 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40 kB 39.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 102 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 143 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 163 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 174 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 184 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 194 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 204 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 215 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 225 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 235 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 245 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 276 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 286 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 296 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 307 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 317 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 327 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 337 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 348 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 356 kB 30.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n","Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.62.3)\n","Collecting slicer==0.0.7\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n","Building wheels for collected packages: shap\n","  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491646 sha256=c99ff063d6d8cc8300f13935393d47a48fa203896e84fc8b7a477c058c5469ab\n","  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n","Successfully built shap\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.39.0 slicer-0.0.7\n"]}]},{"cell_type":"code","metadata":{"id":"EsRlHNJpW12t"},"source":["#Make the necessary imports\n","\n","from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.initializers import Constant\n","from tensorflow.keras.utils import to_categorical\n","from keras import regularizers\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from zipfile import ZipFile\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","import warnings\n","import os\n","\n","warnings.filterwarnings(\"ignore\") "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KQXrrS3XK57","executionInfo":{"status":"ok","timestamp":1634743021616,"user_tz":-210,"elapsed":1749,"user":{"displayName":"Kasra Sadeghianpoor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-M-IQfCdV1sDtftP0aqLtsS4eCJ8Pzr5Ka4Ee=s64","userId":"12889102283785354184"}},"outputId":"a5cce9a9-1668-4e49-91e1-f257ed5b1b77"},"source":["nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"punkt\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"OToXnIOs3WfV"},"source":["MAX_SEQUENCE_LENGTH = 1000\n","MAX_NUM_WORDS = 20000 \n","EMBEDDING_DIM = 100 \n","VALIDATION_SPLIT = 0.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"fWA0kThB4PC4","executionInfo":{"status":"ok","timestamp":1634743026055,"user_tz":-210,"elapsed":2318,"user":{"displayName":"Kasra Sadeghianpoor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-M-IQfCdV1sDtftP0aqLtsS4eCJ8Pzr5Ka4Ee=s64","userId":"12889102283785354184"}},"outputId":"dbc7cadd-9305-40f0-f41c-c757c20de5f4"},"source":["data = pd.read_csv(\"cleaned_data.csv\")\n","data = data.drop(\"Unnamed: 0\" , axis=1)\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>one review mention watch oz episod hook right ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wonder littl product film techniqu unassum old...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>thought wonder way spend time hot summer weeke...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>basic famili littl boy jake think zombi closet...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>petter mattei love time money visual stun film...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment\n","0  one review mention watch oz episod hook right ...          1\n","1  wonder littl product film techniqu unassum old...          1\n","2  thought wonder way spend time hot summer weeke...          1\n","3  basic famili littl boy jake think zombi closet...          0\n","4  petter mattei love time money visual stun film...          1"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Kf9W6nC4S0g","executionInfo":{"status":"ok","timestamp":1634743362773,"user_tz":-210,"elapsed":584,"user":{"displayName":"Kasra Sadeghianpoor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-M-IQfCdV1sDtftP0aqLtsS4eCJ8Pzr5Ka4Ee=s64","userId":"12889102283785354184"}},"outputId":"3cd7ed01-ee35-487d-a2a3-db1491b15cb3"},"source":["data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 2)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"n80zqzid4qXI"},"source":["X = data.drop(\"sentiment\", axis = 1)\n","y = data[\"sentiment\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jHqX94r4l_s"},"source":["from sklearn.model_selection import train_test_split\n","X_train , X_test, y_train, y_test = train_test_split(X, y, shuffle=True,\n","                                                     test_size=0.2, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RxJCWSe5tJk"},"source":["# turn the splitted datasets into list format in order to \n","# apply Embedding operations on them\n","\n","X_train = X_train[\"review\"].to_list()\n","X_test = X_test[\"review\"].to_list()\n","y_train = y_train.to_list()\n","y_test = y_test.to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVX-ec4O4HSq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634743374241,"user_tz":-210,"elapsed":7480,"user":{"displayName":"Kasra Sadeghianpoor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-M-IQfCdV1sDtftP0aqLtsS4eCJ8Pzr5Ka4Ee=s64","userId":"12889102283785354184"}},"outputId":"85037e66-c7ef-4c16-b280-1df1b823b6a9"},"source":["# instantiate the tokenizer object and fit it on the \n","# training set\n","tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n","tokenizer.fit_on_texts(X_train)\n","\n","# convert each and every sentences in training and test sets\n","# into sequence of numbers in order to apply further Embedding operations\n","train_sequences = tokenizer.texts_to_sequences(X_train)\n","test_sequences = tokenizer.texts_to_sequences(X_test)\n","\n","word_index = tokenizer.word_index\n","\n","print(\"Found {} unique tokens...\".format(len(word_index)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 62842 unique tokens...\n"]}]},{"cell_type":"code","metadata":{"id":"nyPOqCaw5psF"},"source":["# apply 'pre' padding of 0s on the train and test sets to \n","# set the length of all of the sentences into a specific value\n","# in order to fed them into neural network \n","trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","trainvalid_labels = to_categorical(np.asarray(y_train))\n","test_labels = to_categorical(np.asarray(y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2Dn3N5w7b0z","executionInfo":{"status":"ok","timestamp":1634718017082,"user_tz":-210,"elapsed":7,"user":{"displayName":"Kasra Sadeghianpoor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-M-IQfCdV1sDtftP0aqLtsS4eCJ8Pzr5Ka4Ee=s64","userId":"12889102283785354184"}},"outputId":"74495b89-617e-45d0-e8e0-a9c66855d033"},"source":["# get the indeces of trainig set, shuffle it and split\n","# training set into training and validation sets\n","indeces = np.arange(trainvalid_data.shape[0])\n","np.random.shuffle(indeces)\n","trainvalid_data = trainvalid_data[indeces]\n","trainvalid_labels = trainvalid_labels[indeces]\n","num_validation_steps = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n","X_train = trainvalid_data[:-num_validation_steps]\n","y_train = trainvalid_labels[:-num_validation_steps]\n","X_val = trainvalid_data[-num_validation_steps:]\n","y_val= trainvalid_labels[-num_validation_steps:]\n","\n","print('Splitting the train data into train and valid is done')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Splitting the train data into train and valid is done\n"]}]},{"cell_type":"code","metadata":{"id":"35rFBwXqCPl9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634718101069,"user_tz":-210,"elapsed":83991,"user":{"displayName":"Kasra Sadeghianpoor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-M-IQfCdV1sDtftP0aqLtsS4eCJ8Pzr5Ka4Ee=s64","userId":"12889102283785354184"}},"outputId":"daa38762-b025-4418-bf28-422630c830ec"},"source":["cnnmodel = Sequential()\n","cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n","cnnmodel.add(Conv1D(64, 5, activation='relu'))\n","cnnmodel.add(MaxPooling1D(5))\n","cnnmodel.add(GlobalMaxPooling1D())\n","cnnmodel.add(Dense(64, activation='relu',\n","             kernel_regularizer=regularizers.l2(0.1)))\n","cnnmodel.add(Dropout(0.5))\n","cnnmodel.add(Dense(2, activation='softmax'))\n","\n","cnnmodel.compile(loss=\"categorical_crossentropy\",\n","              optimizer=\"adam\",\n","              metrics=[\"acc\"])\n","\n","# train the model. tune to validation set. \n","cnnmodel.fit(X_train, y_train,\n","          batch_size=16,\n","          epochs=3, validation_data=(X_val, y_val))\n","\n","# evaluate on test set:\n","score, acc = cnnmodel.evaluate(test_data, test_labels)\n","print('Test accuracy with CNN:', acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","2000/2000 [==============================] - 29s 13ms/step - loss: 0.6603 - acc: 0.7945 - val_loss: 0.3471 - val_acc: 0.8630\n","Epoch 2/3\n","2000/2000 [==============================] - 27s 13ms/step - loss: 0.2758 - acc: 0.9026 - val_loss: 0.3302 - val_acc: 0.8765\n","Epoch 3/3\n","2000/2000 [==============================] - 25s 13ms/step - loss: 0.1479 - acc: 0.9608 - val_loss: 0.4244 - val_acc: 0.8472\n","313/313 [==============================] - 2s 6ms/step - loss: 0.4039 - acc: 0.8551\n","Test accuracy with CNN: 0.8550999760627747\n"]}]},{"cell_type":"code","metadata":{"id":"xjJuNI1gOqOb"},"source":["cnnmodel.save(\"model/cnn-model.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vz9gYXbkY7ta","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634743382119,"user_tz":-210,"elapsed":1096,"user":{"displayName":"Kasra Sadeghianpoor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-M-IQfCdV1sDtftP0aqLtsS4eCJ8Pzr5Ka4Ee=s64","userId":"12889102283785354184"}},"outputId":"bd23bee9-dfca-47ef-a03c-ad0b33f4d990"},"source":["text = \"it is a bad movie overall.\"\n","ps = PorterStemmer()\n","CLEAN = re.compile(\"<.*?>\")   # to remove everything between \"<>\" \n","result = re.sub(CLEAN, \" \", text)\n","result = re.sub(\"[^a-zA-Z]\" , \" \" , result)\n","result = result.lower()\n","result = result.split()   # to break sentences into words\n","word = [ps.stem(word) for word in result if word not in stopwords.words(\"english\")]\n","result = \" \".join(word)\n","tokens = tokenizer.texts_to_sequences([result])\n","sent = pad_sequences(tokens, maxlen=MAX_SEQUENCE_LENGTH)\n","pred = cnnmodel.predict(np.array(sent))\n","print(pred)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.8123285  0.18767147]]\n"]}]}]}